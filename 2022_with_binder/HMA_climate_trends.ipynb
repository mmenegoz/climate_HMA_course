{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51dcbe7",
   "metadata": {},
   "source": [
    "**High-Mountain Asia climate analysis based on gridded observations**\n",
    "\n",
    "**Katmandhu course, October, 2022**\n",
    "\n",
    "Topography, temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e777c",
   "metadata": {},
   "source": [
    "**1. Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96934af5-1143-498b-aca5-a45e0701e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar as cld\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as plot # New plot library (https://proplot.readthedocs.io/en/latest/)\n",
    "plot.rc['savefig.dpi'] = 300 # 1200 is too big! #https://proplot.readthedocs.io/en/latest/basics.html#Creating-figures\n",
    "from scipy import stats\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968c608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the data files?\n",
    "path=\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03498fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain study: HMA\n",
    "lon1=60;lon2=110\n",
    "lat1=25;lat2=45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19dd3f",
   "metadata": {},
   "source": [
    "**2. Open temperature data file (CRU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69df479d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/Users/mmenegoz/cours/nepal_2022/climate_HMA_course/data/HMA_cru_ts4.06.1901.2021.tmp.dat.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/lru_cache.py:53\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/mmenegoz/cours/nepal_2022/climate_HMA_course/data/HMA_cru_ts4.06.1901.2021.tmp.dat.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CRU dataset (https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.06/cruts.2205201912.v4.06/tmp/cru_ts4.06.1901.2021.tmp.dat.nc.gz)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://www.nature.com/articles/s41597-020-0453-3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Here we download only a part of the file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHMA_cru_ts4.06.1901.2021.tmp.dat.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/api.py:497\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    486\u001b[0m     decode_cf,\n\u001b[1;32m    487\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    496\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 497\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    504\u001b[0m     backend_ds,\n\u001b[1;32m    505\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:551\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    532\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[1;32m    550\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 551\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:380\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    374\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    375\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    376\u001b[0m )\n\u001b[1;32m    377\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    378\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:328\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:389\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:383\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    384\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/file_manager.py:187\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/HMA_env/lib/python3.8/site-packages/xarray/backends/file_manager.py:205\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    203\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 205\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overriden when opened again\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2330\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:1948\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/Users/mmenegoz/cours/nepal_2022/climate_HMA_course/data/HMA_cru_ts4.06.1901.2021.tmp.dat.nc'"
     ]
    }
   ],
   "source": [
    "# CRU dataset (https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.06/cruts.2205201912.v4.06/tmp/cru_ts4.06.1901.2021.tmp.dat.nc.gz)\n",
    "# https://www.nature.com/articles/s41597-020-0453-3\n",
    "# Here we download only a part of the file\n",
    "file='HMA_cru_ts4.06.1901.2021.tmp.dat.nc'\n",
    "ds = xr.open_dataset(os.path.join(path,file))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5111b5c",
   "metadata": {},
   "source": [
    "**3. Extracting regional data and computing seasonal means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data over HMA\n",
    "longitudes=slice(lon1,lon2)\n",
    "latitudes=slice(lat1,lat2)\n",
    "temp_HMA=ds.sel(lat=latitudes,lon=longitudes)\n",
    "temp_HMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for seasonal mean\n",
    "def season_mean(ds, calendar=\"standard\"):\n",
    "    # Make a DataArray with the number of days in each month, size = len(time)\n",
    "    month_length = ds.time.dt.days_in_month\n",
    "\n",
    "    # Calculate the weights by grouping by 'time.season'\n",
    "    weights = (\n",
    "        month_length.groupby(\"time.season\") / month_length.groupby(\"time.season\").sum()\n",
    "    )\n",
    "\n",
    "    # Test that the sum of the weights for each season is 1.0\n",
    "    np.testing.assert_allclose(weights.groupby(\"time.season\").sum().values, np.ones(4))\n",
    "\n",
    "    # Calculate the weighted average\n",
    "    return (ds * weights).groupby(\"time.season\").sum(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing seasonal mean and ordering properly the seasons\n",
    "seasonal_mean=season_mean(temp_HMA.tmp).sortby(xr.DataArray(['DJF','MAM','JJA', 'SON'],dims=['season']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70743d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa541ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels of temperature for the plot\n",
    "levels=plot.arange(-34,34,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing the map of the seasonal mean, excluding the borders of the domain.\n",
    "f, axs = plot.subplots(proj='cyl',ncols=2, nrows=2, share=1, axwidth=5)\n",
    "for i, ax in enumerate(axs):\n",
    "    print('i='+str(i))\n",
    "    print('ax='+str(ax))\n",
    "    m = ax.pcolormesh(\n",
    "        temp_HMA.lon,temp_HMA.lat,\n",
    "        seasonal_mean[i,:,:],\n",
    "        levels=levels,\n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    ax.format(title=seasonal_mean.season.data[i],large='20px')\n",
    "    \n",
    "f.colorbar(m, label= '°C')\n",
    "\n",
    "axs.format(\n",
    "    geogridlinewidth=0.1, geogridcolor='gray8', geogridalpha=0.5, labels=True, \n",
    "    coast=True, ocean=False, oceancolor='gray3', borders=True,\n",
    "    suptitle=\"Seasonal mean of temperature, CRU observations\",\n",
    "    lonlines=4, latlines=4, abc=False, latlim=[lat1,lat2],lonlim=[lon1,lon2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73dc73",
   "metadata": {},
   "source": [
    "**4. Trends**\n",
    "\n",
    "Choose the period that you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period over which computing the trends\n",
    "date1=\"1901-01\"; date2=\"2021-12\"\n",
    "date3=\"2022-12\" # One year after date2\n",
    "dates=pd.date_range(start=date1, end=date3, freq='Y')\n",
    "dates.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60294dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract seasonal timeseries\n",
    "Temp=temp_HMA.tmp.sel(time=slice(date1,date2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d08916",
   "metadata": {},
   "source": [
    "**4.1 Check the trend and its level of significance in the location of your choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ec4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location study\n",
    "lon_loc=87; lat_loc=28 # Everest location!!!\n",
    "#lon_loc=84; lat_loc=28\n",
    "#lon_loc2=72; lat_loc2=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of the data at the point\n",
    "temp_loc=Temp.sel(lon=lon_loc, lat=lat_loc, method=\"nearest\")\n",
    "#temp_loc2=Temp.sel(lon=lon_loc2, lat=lat_loc2, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa873b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty array with the seasonal data\n",
    "seasonal_T_loc=np.empty(shape=(int(temp_loc.shape[0]/12),4), dtype=float)\n",
    "seasonal_T_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eeb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal spatial data\n",
    "seasonal_T_loc [:,0] = temp_loc.where(Temp['time.season'] == 'DJF').groupby('time.year').mean(dim='time')\n",
    "seasonal_T_loc [:,1] = temp_loc.where(Temp['time.season'] == 'MAM').groupby('time.year').mean(dim='time')\n",
    "seasonal_T_loc [:,2] = temp_loc.where(Temp['time.season'] == 'JJA').groupby('time.year').mean(dim='time')\n",
    "seasonal_T_loc [:,3] = temp_loc.where(Temp['time.season'] == 'SON').groupby('time.year').mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing trends\n",
    "slope_T_loc=np.full(seasonal_T_loc.shape[1:4],np.nan)\n",
    "pvalue_T_loc=np.full(seasonal_T_loc.shape[1:4],np.nan)\n",
    "intercept_T_loc=np.full(seasonal_T_loc.shape[1:4],np.nan)\n",
    "for season in range(4):\n",
    "    linregress_T_loc = stats.linregress(range(seasonal_T_loc.shape[0]-1), seasonal_T_loc[1:,season])\n",
    "    slope_T_loc[season] = linregress_T_loc.slope\n",
    "    intercept_T_loc[season] = linregress_T_loc.intercept\n",
    "    pvalue_T_loc[season] = linregress_T_loc.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_T_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62efbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(0,dates.shape[0])\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_T_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a34cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting temperature timeseries\n",
    "f, axs = plot.subplots(ncols=2, nrows=2)\n",
    "for i, ax in enumerate(axs):\n",
    "    print('i='+str(i))\n",
    "    print('ax='+str(ax))\n",
    "    if pvalue_T_loc[i]<0.05:\n",
    "        signif='solid'\n",
    "    else:\n",
    "        signif='dashed'\n",
    "    m = ax.scatter(dates,seasonal_T_loc[:,i]-np.mean(seasonal_T_loc[:,i]))\n",
    "    m = ax.plot(dates,intercept_T_loc[i]+slope_T_loc[i]*index-np.mean(seasonal_T_loc[:,i]),linestyle=signif)\n",
    "    ax.format(title=f\"{seasonal_mean.season.data[i]} trend={slope_T_loc[i]*10:.2f}°C.decade-1; pvalue={pvalue_T_loc[i]:.2g}\",large='10px')\n",
    "    #ax.format(title=seasonal_mean.season.data[i]+' trend='+str(round(slope_T_loc[i]*10,2))+'°C.decade-1; '+'pvalue='+str(round(pvalue_T_loc[i],3)),large='10px')\n",
    "\n",
    "axs.format(\n",
    "    suptitle='Seasonal temperature at lon='+str(lon_loc)+' lat='+str(lat_loc),\n",
    "    xlabel='year',\n",
    "    ylabel='temperature anomaly'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dae2bc",
   "metadata": {},
   "source": [
    "**4.2. HMA trend maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the total number of years\n",
    "int(Temp.shape[0]/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty array with the total seasonal data\n",
    "seasonal_T=np.empty(shape=(int(Temp.shape[0]/12),4,Temp.shape[1],Temp.shape[2]), dtype=float)\n",
    "seasonal_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f97fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temp.where(Temp['time.season'] == 'DJF').groupby('time.year').mean(dim='time').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal spatial data\n",
    "seasonal_T [:,0,:,:] = Temp.where(Temp['time.season'] == 'DJF').groupby('time.year').mean(dim='time')\n",
    "seasonal_T [:,1,:,:] = Temp.where(Temp['time.season'] == 'MAM').groupby('time.year').mean(dim='time')\n",
    "seasonal_T [:,2,:,:] = Temp.where(Temp['time.season'] == 'JJA').groupby('time.year').mean(dim='time')\n",
    "seasonal_T [:,3,:,:] = Temp.where(Temp['time.season'] == 'SON').groupby('time.year').mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999db67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_T.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing trends\n",
    "slope_T=np.full(seasonal_T.shape[1:4],np.nan)\n",
    "pvalue_T=np.full(seasonal_T.shape[1:4],np.nan)\n",
    "\n",
    "for lon in tqdm(range(100)):\n",
    "    for lat in range(seasonal_T.shape[2]):\n",
    "        for season in range(4):\n",
    "            linregress_T = stats.linregress(range(seasonal_T.shape[0]-1), seasonal_T[1:,season,lat,lon])\n",
    "            slope_T[season][lat][lon] = linregress_T.slope\n",
    "            pvalue_T[season][lat][lon] = linregress_T.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking non-significative signals\n",
    "signif=np.where(pvalue_T<0.05,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56959eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels of temperature for the plot\n",
    "levels=plot.arange(-0.3,0.3,0.05)\n",
    "factor=10 # degree per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing the map of the trends, excluding the borders of the domain.\n",
    "f, axs = plot.subplots(proj='cyl',ncols=2, nrows=2, share=1, axwidth=5)\n",
    "for i, ax in enumerate(axs):\n",
    "    print('i='+str(i))\n",
    "    print('ax='+str(ax))\n",
    "    m = ax.pcolormesh(\n",
    "        temp_HMA.lon,temp_HMA.lat,\n",
    "        slope_T[i,:,:]*factor,\n",
    "        levels=levels,\n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    ax.format(title=seasonal_mean.season.data[i],large='20px')\n",
    "    \n",
    "    p = ax.contourf(\n",
    "    temp_HMA.lon,temp_HMA.lat,\n",
    "    signif[i,:,:],\n",
    "    hatches=[\"\", \".\"], alpha=0\n",
    "    )\n",
    "    \n",
    "f.colorbar(m, label= '°C.decade-1')\n",
    "\n",
    "axs.format(\n",
    "    geogridlinewidth=0.1, geogridcolor='gray8', geogridalpha=0.5, labels=True, \n",
    "    coast=True, ocean=False, oceancolor='gray3', borders=True,\n",
    "    suptitle=\"Seasonal trend of temperature, CRU dataset over \"+date1+\" - \"+date2,\n",
    "    lonlines=4, latlines=4, abc=False, latlim=[lat1,lat2],lonlim=[lon1,lon2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c603e",
   "metadata": {},
   "source": [
    "**Now, you can invent your own analysis, e.g:**\n",
    "* Computing trends where you want and over any period.\n",
    "* Computing trends as a function of the elevation using the topography in the file below.\n",
    "* Etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ab471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topography at 0.25° (http://research.jisao.washington.edu/data_sets/elevation/)\n",
    "# -> 0.25-degree latitude-longitude resolution elevation (TBASE)\n",
    "file_topo=\"elev.0.25-deg.nc\"\n",
    "ds_topo = xr.open_dataset(os.path.join(path,file_topo))\n",
    "ds_topo # ds_topo as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data over HMA\n",
    "# be careful to reversed latitudes in the original file\n",
    "topo_HMA=ds_topo.isel(time=0).sel(lon=slice(lon1,lon2),lat=slice(lat2,lat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "m=ax.pcolormesh(topo_HMA.lon,topo_HMA.lat,topo_HMA.data)\n",
    "fig.colorbar(m,label= 'm.asl')\n",
    "ax.set_title('Topography TBASE with a 0.25° resolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ddb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation of the topography on the CRU grid\n",
    "ds_topo_HMA_out=topo_HMA.interp(lat=temp_HMA.lat,lon=temp_HMA.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "m=ax.pcolormesh(ds_topo_HMA_out.lon,ds_topo_HMA_out.lat,ds_topo_HMA_out.data)\n",
    "fig.colorbar(m,label= 'm.asl')\n",
    "ax.set_title('Topography TBASE interpolated on a 0.5° resolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2962bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels of temperature for the plot\n",
    "#levels=np.arange(-0.5,0.5,0.05)\n",
    "factor_trend=10 # degree per decade\n",
    "bins=100\n",
    "levels=np.logspace(0,1,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only the elevation area > limit m.asl\n",
    "limit=1000\n",
    "topo_high=ds_topo_HMA_out.data.where(ds_topo_HMA_out.data > limit)\n",
    "trends_high=np.zeros(slope_T.shape)\n",
    "for i in np.arange(4):\n",
    "    trends_high[i,:,:]=np.where(ds_topo_HMA_out.data > limit,slope_T[i,:,:],np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117c092",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Plotting trends as a function of elevation\n",
    "f, axs = plot.subplots(ncols=2, nrows=2,share=1)\n",
    "for i, ax in enumerate(axs):\n",
    "    print('i='+str(i))\n",
    "    print('ax='+str(ax))\n",
    "    m = ax.scatter(topo_high.values.flatten(),trends_high[i,:,:].flatten()*factor_trend,alpha=0.1)\n",
    "    #m = ax.hist2d(topo_high.values.flatten(),trends_high[i,:,:].flatten()*factor_trend,bins=bins,cmap='Spectral_r',levels=levels,cmin=1)\n",
    "    #m = ax.hist2d(ds_topo_HMA_out.data.values.flatten(),slope_T[i,:,:].flatten()*factor_trend,bins=bins,cmap='Spectral_r',levels=levels,cmin=1)\n",
    "    #m = ax.scatter(ds_topo_HMA_out.data,slope_T[i,:,:]*factor_trend,alpha=0.1)\n",
    "    ax.format(title=seasonal_mean.season.data[i],large='15px',ylim=(-0.05,0.25))\n",
    "\n",
    "    #f.colorbar(m[3],ticks=np.logspace(0,1,2))    \n",
    "\n",
    "axs.format(\n",
    "    suptitle='Temperature trend as a function of the topography',\n",
    "    xlabel='Elevation (m. asl)',\n",
    "    ylabel='trends (°C.decade-1)',\n",
    "    large='15px'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77362ae4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:HMA_env] *",
   "language": "python",
   "name": "conda-env-HMA_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
